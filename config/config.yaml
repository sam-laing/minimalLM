## CONFIG file
## change paths with your own!

deterministic: True
seed: [900, 901, 902, 903, 904] # , 205, 206, 207, 208, 209, 210, 211, 212, 213, 214]
sampler_seed: 66
one_seeded: true

trainset_path: "/fast/slaing/data/lm/slim_pajama/new_sp_15B_tokens/train"
vocab_size: 50280
seq_len: 2048
sampler: 'sequential'
sampler_seed: null
num_workers: 4

eval: True
validset_path: "/fast/slaing/data/lm/slim_pajama/new_valid/validation" 
eval_every_steps: 99999999999
eval_final: True

model: 'transformer'
d_model: 768
mlp_class: 'glu'
expand: '8/3'
n_layers: 12
n_heads: 12
rms_norm: True
tie_embeddings: False
torch_compile: True

# note: set one of these two
# note: step budget=token_budget/(seq_len * micro_batch_size * grad_accumulation_steps)
steps_budget: 4800

# note: this is micro batch size if grad_accumulation_steps>1
# note: with ddp, effective batch size = batch_size * grad_accumulation_steps * ddp_world_size
micro_batch_size: 32
grad_accumulation_steps: 8

# note: choose between {float32, float16, bfloat16}
# note: float16 data type will automatically use a GradScaler
dtype: 'bfloat16'

optim: "custom_adamw"
fused_optim: False 
lr: 3.e-3
weight_decay: 0.1
beta1: 0.91
beta2: 0.97
dampening: 0.1
eps: 1.e-8
grad_clip: 1.0
do_bias_correction: [true, false]
zero_init: [true, false] #whether to initialise the moments to zero (true) or to g, g^2 (false)
adam_to_sgd_ratio: 1.0

scheduler: "warmup_cosine"
warmup_steps: 0.1
cooldown_steps: null
lr_start: 1.e-6
lr_end: 1.e-6
lr_end_pct: null

log_every_steps: 1
print_progress: True

use_wandb: True
wandb_project: 'llm_bias_correction'
wandb_dir: '/fast/slaing/logs/llm/wandb'
wandb_run_name: "bias_correction_test" 
exp_name: 'tr_check_not_fused'
out_dir: '/fast/slaing/exp/llm'
wandb_api_key: "0b4cb5f85aad923fbaec987675544b3d4530b0e4"
over_write: True

resume: False
resume_micro_step: null
resume_exp_name: null

save_last_checkpoint: True
save_intermediate_checkpoints: False
save_every_steps: null

track_moments: False
moment_plots: /fast/slaing/adam_moment_plots/llm
