## CONFIG file
## change paths with your own!

determinisitc: False
seed: 999

trainset_path: "/fast/slaing/data/lm/slim_pajama/new_sp_3B_tokens/train"
vocab_size: 50280
seq_len: 2048
sampler: 'sequential'
sampler_seed: null
num_workers: 4

eval: True
validset_path: "/fast/slaing/data/lm/slim_pajama/new_valid/validation" 
eval_every_steps: 20

model: 'transformer'
d_model: 768
mlp_class: 'glu'
expand: '8/3'
n_layers: 12
n_heads: 12
rms_norm: True
tie_embeddings: False
torch_compile: True

# note: set one of these two
# note: step budget=token_budget/(seq_len * micro_batch_size * grad_accumulation_steps)
steps_budget: 4800

# note: this is micro batch size if grad_accumulation_steps>1
# note: with ddp, effective batch size = batch_size * grad_accumulation_steps * ddp_world_size
micro_batch_size: 32
grad_accumulation_steps: 8

# note: choose between {float32, float16, bfloat16}
# note: float16 data type will automatically use a GradScaler
dtype: 'bfloat16'

optim: ["adamw"]
fused_optim: [True] 
lr: [5.e-4, 1.e-3, 5.e-3] 
weight_decay: [0.1, 0.01]
beta1: [0.9]
dampening: [0.1]
beta2: [0.99]
eps: [1.e-8]
grad_clip: [1.0]

scheduler: ['warmup_cosine']
warmup_steps: [0.1]
cooldown_steps: [null]
lr_start: [0.0]
lr_end: [1.e-5]
lr_end_pct: [null]

log_every_steps: [1]
print_progress: [True]

use_wandb: [True]
wandb_project: ['slim_pj_3B_4800steps_gradacc8']
wandb_dir: ['/fast/slaing/logs/slim_pj_3B/wandb']
#exp_name: ['tr_check']
out_dir: ['/fast/slaing/exp/slim_pj_3B']
wandb_api_key: ["0b4cb5f85aad923fbaec987675544b3d4530b0e4"]
over_write: [True]

resume: [False]
resume_micro_step: [null]
resume_exp_name: [null]

save_last_checkpoint: [True]
save_intermediate_checkpoints: [False]
save_every_steps: [null]